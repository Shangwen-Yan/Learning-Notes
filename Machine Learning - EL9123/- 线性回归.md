一元线性回归
======

xm = np.mean(x)

    ym = np.mean(y)
    syy = np.mean((y-ym)\*\*2)
    syx = np.mean((y-ym)\*(x-xm))
    sxx = np.mean((x-xm)\*\*2)
    beta1 = syx/sxx
    beta0 = ym - beta1\*xm
    print("xbar={0:7.2f}, ybar={1:7.2f}".format(xm,ym))
    print("sxx={0:7.2f}, syy={1:7.2f}".format(np.sqrt(sxx),np.sqrt(syy)))
    print("beta0={0:7.2f}, beta1={1:7.2f}".format(beta0,beta1))

    yhat=beta0+beta1\*x RSS = np.mean((y-yhat)\*\*2) print("RSS = {0:7.2f}".format(RSS))

多元线性回归
======

用sklearn包：
----------

    regr = linear\_model.LinearRegression()
    regr.fit(X\_tr,y\_tr)

    regr.coef\_

    y\_tr\_pred = regr.predict(X\_tr) RSS = np.mean((y\_tr\_pred-y\_tr)\*\*2)/(np.std(y\_tr)\*\*2) print("Normalized RSS={0:f}".format(RSS))

    plt.scatter(y\_test,y\_test\_pred) plt.plot([0,350],[0,350],'r')

用np包
----

    ones = np.ones((ns\_train,1)) A = np.hstack((ones,X\_tr)) A.shape

    out = np.linalg.lstsq(A,y\_tr) beta = out[0]



